{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\layal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\layal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary packages + initializing settings\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "nltk.download([\"stopwords\",\"vader_lexicon\"])\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option(\"display.max_rows\",5)\n",
    "\n",
    "# Loading data\n",
    "# training_data = pd.read_csv(\"./twitter_training.csv\", index_col=0, names=[\"Index\",\"Entity\",\"Sentiment\",\"Tweet\"])\n",
    "testing_data = pd.read_csv(\"./twitter_validation.csv\", index_col=0, names=[\"Index\",\"Entity\",\"Sentiment\",\"Tweet\"])\n",
    "\n",
    "# Outputting first 5 rows of training data\n",
    "# training_data.head()\n",
    "# Outputting first 5 rows of testing data\n",
    "# testing_data.head()\n",
    "\n",
    "    #removing_mentions = r'@[A-Za-z0-9_]+'\n",
    "    #removing_links = r'httpss?://[^ ]+'\n",
    "    #removing_links_mentions = r'|'.join((removing_mentions,removing_links))\n",
    "    #removing_www = r'www.[^ ]+'\n",
    "\n",
    "indices = testing_data.index.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(tweet):\n",
    "\n",
    "    removing_mentions = r'@[A-Za-z0-9_]+'\n",
    "\n",
    "    tweet = re.sub(removing_mentions, '', tweet)\n",
    "\n",
    "    tokens = tweet.split()\n",
    "    table = str.maketrans(\" \",\" \",punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    token = [w for w in tokens if w.isalpha()]\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    new_tweet = [w for w in token if w.lower() not in stop_words]\n",
    "\n",
    "    return ' '.join(new_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index\n",
       "3364    I mentioned on Facebook that I was struggling ...\n",
       "352     BBC News - Amazon boss Jeff Bezos rejects clai...\n",
       "                              ...                        \n",
       "8069    Bought a fraction of Microsoft today. Small wins.\n",
       "6960    Johnson & Johnson to stop selling talc baby po...\n",
       "Name: Tweet, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_data.Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The iPhone 12 needs to hurry up cause I need to get it with @Verizon cause @sprint is trash ever since the merger with @TMobile! Like my speed is from the Middle Ages!! #iPhone12'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing tweet cleaner\n",
    "idx = indices[900]\n",
    "\n",
    "# Original input\n",
    "testing_data.Tweet.loc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'iPhone needs hurry cause need get cause trash ever since merger Like speed Middle Ages'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean output\n",
    "clean_tweet(testing_data.Tweet.loc[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_tweets = []\n",
    "\n",
    "for idx in testing_data.index.values.tolist():\n",
    "    token_tweets.append(clean_tweet(testing_data.Tweet.loc[idx]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Wow', 3),\n",
       " ('Check video LeagueofLegends Captured Outplayed', 3),\n",
       " ('Red Dead Redemption', 2)]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_dist_tweet = nltk.FreqDist(token_tweets)\n",
    "freq_dist_tweet.most_common(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.196, 'neu': 0.804, 'pos': 0.0, 'compound': -0.296}\n",
      "Neutral\n"
     ]
    }
   ],
   "source": [
    "# Testing sentiment analyzer\n",
    "idx = 6960\n",
    "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "print(sentiment_analyzer.polarity_scores(clean_tweet(testing_data.Tweet.loc[idx])))\n",
    "print(testing_data.Sentiment.loc[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322 228 450\n",
      "277 266 457\n"
     ]
    }
   ],
   "source": [
    "all_sentiment = []\n",
    "count = 0\n",
    "\n",
    "pos_count_vader = 0\n",
    "neg_count_vader = 0\n",
    "neu_count_vader = 0\n",
    "\n",
    "pos_count_true = 0\n",
    "neg_count_true = 0\n",
    "neu_count_true = 0\n",
    "\n",
    "\n",
    "for idx in testing_data.index.values.tolist():\n",
    "    input_tweet = clean_tweet(testing_data.Tweet.loc[idx])\n",
    "    all_sentiment.append(sentiment_analyzer.polarity_scores(input_tweet))\n",
    "    \n",
    "    score = all_sentiment[count]\n",
    "\n",
    "    # Not sure how to threshold positive vs negative vs neutral score\n",
    "    if score['compound'] > 0.5:\n",
    "        pos_count_vader+=1\n",
    "    elif score['compound'] < -0.5:\n",
    "        neg_count_vader+=1\n",
    "    else:\n",
    "        neu_count_vader+=1\n",
    "    \n",
    "    if testing_data.Sentiment.loc[idx]=='Positive':\n",
    "        pos_count_true+=1\n",
    "    elif testing_data.Sentiment.loc[idx]=='Negative':\n",
    "        neg_count_true+=1\n",
    "    else:\n",
    "        neu_count_true+=1\n",
    "\n",
    "    count+=1\n",
    "\n",
    "print(pos_count_vader,neg_count_vader,neu_count_vader)\n",
    "print(pos_count_true,neg_count_true,neu_count_true)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "958cef59ee8b1c1f57a2d9524612d8a965c27517d6cfb9390e4072240344671e"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
